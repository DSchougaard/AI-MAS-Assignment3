\def\year{2015}
%File: formatting-instruction.tex
\documentclass[letterpaper]{article}

% Required Packages
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}

\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

% Section numbers. 
\setcounter{secnumdepth}{2}  


\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}


\nocopyright
\begin{document}
% Title and author information
\title{Mandatory Assignment 3 - Group null\\ 02285 AI \& MAS}
\author{Daniel Schougaard \\ \textit{s103446} \And Kasper Reindahl Rasmussen\\ \textit{s103476} \And Martin V. Ottesen\\ \textit{s060186}}
\maketitle

\begin{abstract}
In this paper we percent a planning  system for solving Pukoban problems in a coordination effort of up to 10 agents. This involves path planning, conflict resolution, multiple heuristics, domain relaxation, distance metrics, partial-order planning and clustering.  It describes how we have used these techniques in practice and attempts to justify the choice of distance calculation metrology, search algorithm and level of goal independence based on selected test cases concluding that in spite of high start cost a good distance map is central to the success of our solution.  We discuss the strengths and weaknesses of our solution based on the completion level results. Finally we describe what would be the logical continuation of the project if we were to continue its development either by improving on existing functionality like algorithm selection or adding entirely new functionality like agent coordination, backtracking and improving the goal decomposition throw ex. goal ordering.
\end{abstract}

\section{Introduction}
	Inspired by the developments of robotics used for transporting assignments to lower the need for humans to carry out these trivial tasks, the challenge is to have multiple robots capable of distributing these tasks among themselves, resulting in one combined multi-agent system capable of efficient and reliable execution of such tasks.\\
	This paper describes a solution to this planing problem on an abstract level, where the hardware of the physical robots have been disregarded and the environments have been relaxed in such a way that they make up grid like patterns. The  moving ententes which will henceforth be referred to as agents are given the ability to push and pull the objects henceforth refereed to as boxes around in the grid map, with the objective of moving these boxes to there destinations called goals, while navigation the obstacles in there way, be they walls, other agents or boxes they are not allowed to move (restrictions are defined by agent/box colour), by enforcing these rules, restrictions and relaxations the resulting problem domain is of a single/multi agent Pukoban nature. 
	\subsection{The solution}
		To solve the problem the level is first analysed mapping all agents, boxes, goals and walls, calculating distances from any cells to all other cells in the space, clustering the goals for the agents and then prioritizing them so that blocking goals can be avoided.\\
		The agents run autonomously with no inter agent communication, the solution is pseudo subgoal independent as an agent does not forget old goals when achieved, new ones are simply added to the list. To make sure that a route can always be found a relaxed search is run first and then an actual search. The planing is done on-line so conflicts are detected when the server objects this results in the invocation of conflict resolution.\\
		If the case of a single agent problem a simpler strategy is used as all boxes are movable by the agent and no other agents can create road blocks no conflicts can occur.


\section{Background}
	The Pukoban problem is closely related to the classical automated planing problem called Sokoban the difference is that you are also allowed to pull the boxes in the Pukoban case. As an automated planing problem we need to calculate action/command sequences to solve the problem/levels these sequences of commands have to be sent to to a provided server application.
	The levels are solved using progression planning, implemented using a best-first heuristic search to explore the state space as the roles in the assignment implies completeness in the levels. The search algorithms we have found interesting to look at are the AStar and Greedy as they are well documented and suit our purpose well.
	\subsection{Multi-agent System}
		As part of the solution we have to take the requirement for multiple agents into consideration, this means that some of the basic assumptions in automatic planing have to be re-evaluated as these are normally single-agent by default.
		Multi-agent systems are generality neither static or fully observable this makes offline planing problematic as centralised planing is very computationally heavy, an alternative to this is to make the agents work autonomously. Full autonomy  however opens op the possibility of agents obstructing each-other and the introduction of color restrictions on which agents are able to move which boxes can make some goals initially unattainable, this opens up the requirement for some sort of conflict resolution in the form of online replanning.
	\subsection{Relaxation}
		In order to make sure that an agent has a plan to carry out, even though none of it's goals are actually attainable a relaxation of the problem may be necessary to relax the search space in some way. The relaxation has to be done in such a the plan is still feasible, so that the time spent on calculation a relaxed solution is not a wast of time and processing power only non-permanent obstacles should be disregarded and  agent and box parameters are final. By following these rules a we can end up with an emulated single agent system.
		Relaxed solution should however only be used if no non relaxed solution can be produced. 
	\subsection{Partial-order Planing}
		An other useful relaxation is only to plan on a subset of the goals at a time instead of trying to solve them all in one go, at the same time if you only look at one goal at a time the agents might constantly undo attained goals in order to fulfil new ones. In order to make this function optimally ordering of goals is also necessary so that the goals come in an optimal order ~\cite{Subgoals}.
	\subsection{Clustering}
		In order to optimize the levels with multiple agents with the same problem space the goals are decided into clusters. this is done using a center-based K-means algorithm ~\cite{K-Means}.
		
\section{Related Work}
	Sokoban is as stated earlier a classical automated planing problem it was created in 1981 by Hiroyuki Imabayashi, and published in 1982. During it's 33 years a myriad of solutions have been created to solve it.
	The Pukoban and Multi-Agent variations however are not as well documented.
	\subsection{Pukoban}
		In \textsc{motion planning with pull and push moves} ~\cite{Pukoban} is a solution to a single agent to the Pukoban problem. To the single agent implementation this solution has a very similar approach it however uses the Kuhn-Munkres(Hungarian)algorithm, instead of Floyd-Warshall this gives them some overhaed as they have to runn it multiple times.
		~\cite{Pukoban} pairs up the boxes to goals they also analyse for bottlenecks called chock points and clean them for what they call Clogs, they have no partial order planing   so they only have a success rate of 30 on fairly simple levels.
	\subsection{Multi-Agent}
		In \textsc{A Multi-Agent Planning Approach Integrated with Learning Mechanism} ~\cite{Multi-Agent} is a solution to Sokoban problems it uses centralized planing. It is integrates machine learning in order to build up a knowledge base capable of solving complex Sokoban problems. As the problems the self learning system is solving is Sokoban the state-space does not explode in the same way as a Pukoban would, this makes it possible fairly fast to go throw possible combinations and learn what no to do, and be rewarded in accordance to how many goals were attained before the game was deadlocked. It is hard to compare this approach to the one we are using for the Pokuban problem but for this type of problem it should theoretically be more suited as it could with enough time solve a NP-hard problem, and the central control system is a better choice for Sokuban as backtracking is not a possibility.

\section{Methods}
%	\subsection{Brief Overview of Design and Implementation}
%		Reindahl\\
%		the general structure is a multi body approach with preprocessing of the goals.
%		
%		the preprocessing consist of clustering the goals to the agent.
%			Randomization
%		
%		we assume that goals located near each 
%		goal decomposition 
%			subgoal independence
%			adding goals for agents
%		
	The general structure used is a multibody approach with preprocessing. The complexity is lowered by decomposing the problem by assuming that goals of the same colour, located close to each other are related and therefore can be grouped together (using k-clustering). It is also assumed that solving each grouping is independent of the others. One agent is assigned to solve each grouping. Each agent in this system is responsible for solving its own goals independent of the other agents, as far as possible. 
	To lower the complexity further for each grouping the goals are solved cumulative. The order of solving the goals is based on a heuristic. It cumulative solves the goals, to avoid destroying already achieved goals, since they are assumed to be dependent on each other. To find a solution for a set of goals a search is performed. After each search the found plan is executed until it is completed or it fails. If it fails replanning is commenced.

 
	\subsection{The Heuristic}
		%Reindahl \\
		%RELAXATION OF A WABBIT, WABBIT! WABBIT!\\
		A series of heuristics is used, each one to solve a different problem. All the heuristics have that in common, that to enhance the heuristics the distances used, is the shortest path, when the domain is relaxed by removing all movable objects, these values can be precomputed (The All-pairs shortest paths problem).
		
		Of all the heuristic used only two are noteworthy \nameref{Goal1} and \nameref{goal2}, the rest is basically just the distance to or from a point.
		\subsubsection{Goal Heuristic}\label{Goal1}
			The main heuristic used is the one for placing boxes on goals. It consist of two parts, one to ensure progress and one to ensure as little interference as possible with the other agents.
			
			The first part calculates the shortest path from each of the agents assigned goals to the agent, going through a box that matches the goal (boxes already on goals are not used). All these parts are summed together.
			
			The second part is basically a goal count. It works as punishment for moving boxes away from goal fields, even though the goals ar not part of the agents goal condition.
			
			This results in an non-admissible heuristic.
		
		\subsubsection{Goal Selection Heuristic}\label{goal2}
			%Reindahl
			This heuristic is used to to choose the order in which the goals is solved. Goals are selected from a agents grouping mainly based on the shortest distance from a goal to the nearest similar box (not on a goal) and to the agent. The location of a given goal is also taken into consideration so goals likely to be blocked (fx. dead ends and corners) is prioritised higher by a factor.

				
			
	\subsection{Distances in the Map}
		To increase the performance of our heuristics, we have been employing something we've called a ``Distance Map". A distance map is a certain way to calculate distances from one location to another.
			
		One important fact to realise, is that the levels we work on here, can represented as graphs: Each cell is a vertice, $V$, and each connection to another cell is an edge, $E$. With this knowledge in hand, we quickly realised that we could employ various algorithms actually meant to work on graphs. At the time of writing, we've implemented, tested and worked with a distance map generated using the Floyd-Warshall algorithm.

	\subsection{Search}
		\label{sec:search}
		To find a plan to solve a given set of goals, a search is performed. The search is either a Greedy or A* search. A* is used in the single agent world and when the problem is considered small enough, such as moving out of a given path. 
		The search works under the assumption, that nothing changes in the world except what the agent performing the search do. This will sometimes fail on the multi agent levels, and we rely on simply letting the server \emph{tell} the client, that there is a conflict and then replanning is commenced. This is by far the simplest possible way to detect a conflict, but it does inevitably result in a greater overhead. 
		
		
		To avoid using to much time on searches which are impossible or just to long, we employ a sort of double search. By first relaxing the domain, into a subdomain, only containing the agent itself and boxes, it can move. We then search this reduced domain. Using the length of the found route, we get a threshold for determining whether or not the normal search is worth progressing with. This approach rely on the assumption that relaxed search is significantly faster than the normal search. This assumption is however not the case for the single agent level, and it is therefore not performed in that case.
		
		
		
		
		Another thing worth remarking on here is that a randomized approach is used in expanding the search space, which means that a search is often not likely to result in the same plan each time. This even more so the case when combining it with a greedy approach and a non admissible heuristic.




	\subsection{Conflict Resolution}
		Since we're using multiple agents, each planning their own route, conflicts are bound to happen. We've chosen to rely on a method to resolve these somewhat similar to online planning. But first and foremost, we need to describe exactly how we \emph{detect} these conflicts.

		\subsubsection{Detecting the Conflicts}
			Currently we rely on two different methods for detecting if agent is stuck. As was mentioned in section \ref{sec:search}, we employ a double search. Because the threshold is generated from the subdomain, we can assume that should the threshold be exceeded, a conflict must be present.

			The second approach is using something we call a \verb=History=. Due to the way we implemented searching and re-planning, occasionally two agents could find themselves in a situation, where both of them would \textsc{cycle} between two locations -- a so called live-lock. The \verb=History= is a memory of a certain length, containing information about where an agent has been for the past $x$ moves. Should more than a certain fraction of this history be of the same location, a cycle is detected.

		\subsubsection{Types of Conflicts}
			Having detected a conflict, we partition the causes into one of two groups: The cases where it is only a box in the way, and the cases where it is a box. As a rule, all boxes found in the route, is handled first. This is done to avoid cases, where an agent otherwise asked to move a box, is told to get out of the way. An example of this, can be seen below. $0$ is a \verb=BLUE= agent, and $1$ is a \verb=RED= agent. The box $B$ is also \verb=RED=. 
			\begin{verbatim}
				...+++++++++...
				     0 B1
				...+++++++++...
			\end{verbatim}
			In this example, $0$ would like to move right. Obviously, that is impossible, due to the fact that both the agent $1$ and the box $B$ is in the way. Should we chose to handle agents first, $1$ would move out of the way, leaving no one to move the box $1$.

		\subsubsection{Resolving a Box Conflict}
			To move a box out of the way, we first need to identify which agent should ``help". This is done by first trying to find the nearest agent, which currently does not have a plan. If none is found, the nearest agent is simply hijacked. Once the selected agent has reached the box, we employ a very naïve search: We breadth-first search, until both agent and box is out of the route.

		\subsubsection{Resolving an Agent Conflict}
			If it should happen that is an agent that is in the route, this agent is told to get out of the way, \emph{unless} it's plans length, is below a certain threshold. Should the plan be below this threshold, the agent in the way is allowed to finish its current plan, before getting out of the way \emph{(if it is required at that time)}.

		\subsubsection{Resolving a Cycle}
			A cycle is resolved by simply injecting NoOps in one of the agents, enabling the other to re-plan itself out of the problem area. A cycle conflict falls under the category of an Agent Conflict, but is resolved separately.
		

\section{Experiments and Results}
	Throughout the development and implementation of the client for this project, we have tested out various ideas and designs. This sections contains a mix of experiments performed during all stages of development. The names AStar and Greedy will refer to an A* Search and a Greedy approach search, from now on out.

	Tables \ref{table:bench_astar} and \ref{table:bench_greedy} shows benchmarks using a Greedy search and a AStar search, with respectively a real distance map calculated with the Floyd-Warshall algorithm, and a Manhattan distance map. 
	
	\begin{table}
		\begin{tabular}[t]{ l | r | L{1.2cm} | r | L{1.2cm} }
							& 	\multicolumn{2}{c |}{FloydWarshall}	& 	\multicolumn{2}{c}{Manhattan}		\\
			\textbf{Level}	&	\textbf{Steps}	&	\textbf{Time}	&	\textbf{Steps}	&	\textbf{Time} \\
							&					&	\textbf{(ms)}	&					&	\textbf{(ms)} \\
			\hline
			SACrunch		&	104.0			& 	167.0			&	105.4			&	574.8	\\
			SAFirefly		&	100.4			& 	160.6 			&	62.0			&	202.2	\\
			SABispebjerg	&	1520.8			& 	2023.2 			&	1568.0			&	9439.8	\\
			SAGroup42F13	&	375.0			& 	31846.2 		&	481.4			&	756.4	\\
		\end{tabular}
		\caption{Benchmarks using AStar strategy.}
		\label{table:bench_astar}
	\end{table}


	\begin{table}
		\begin{tabular}{ l | r | L{1.2cm} | r | L{1.2cm} }
							& 	\multicolumn{2}{c |}{FloydWarshall}	& 	\multicolumn{2}{c}{Manhattan}		\\
			\textbf{Level}	&	\textbf{Steps}	&	\textbf{Time} 	&	\textbf{Steps}	&	\textbf{Time} \\
							&					&	\textbf{(ms)}	&					&	\textbf{(ms)} \\	
			\hline
			SACrunch		&	106.4			& 	162.2			&	296.6			&	1849.6	\\
			SAFirefly		&	100.4			& 	164.0 			&	98.4			&	155.8	\\
			SABispebjerg	&	1530.0			& 	1913.6 			&	-				&	-		\\
			SAGroup42F13	&	375.0			& 	31110.4 		&	-				&	-		\\
		\end{tabular}
		\caption{Benchmarks using Greedy strategy.}
		\label{table:bench_greedy}
	\end{table}

	\subsection{Testing Methodology}
		All testing is done over the course of 5 iterations, after which the average is calculated. The limits have been set to approximately 2 gigabyte RAM and 5 minutes. The benchmarks in Table \ref{table:bench_astar} and Table \ref{table:bench_greedy} have been run on a system powered by an Intel i5-3570k (Ivy Bridge) processor, equipped with 8 gigabyte 1866MHz DDR3 RAM, running Ubuntu 12.04.
		
		Table \ref{table:bench_indygoals} and Table \ref{table:bench_cumgoals} have been run on a system powered by an Intel i7-3680QM (Ivy Bridge) processor, equipped with 8 gigabyte 1600MHz DDR3 SDRAM, running Windows 8.1.

	\subsection{Comparison of Search Strategies}
		First off, it could seem like our benchmarks are flawed: Where Greedy fails to find a solution using the Manhattan distance, AStar actually manages to find it, cf. table \ref{table:bench_greedy}. This might seem a tad counter-intuitive. But the reason is actually fairly simple:  We apply goal decomposition, as mentioned earlier. Each search is treated as its own. Hence the difference here, stems from respectively Greedy and AStar fares in search across the level. In larger levels with a rather high possible branching factor, the Greedy approach would simply end up reaching the memory limit, before even getting a box \emph{close} to the goal.

		A real interesting thing that is seen on table \ref{table:bench_astar} is, that using the Manhattan distance map, we get a better solution. This is because very odd set of events. We select the goal order based on the distance, cf. \ref{goal2}. When using the Manhattan distance, the inherent flaw in the distance, actually makes the goal heuristic select a \emph{better} goal order.

		Having said that, we can conclude using a Manhattan distance, with Greedy is a horrible choice. Not only does it result in sub-par solutions, but it simply fails to solve both \verb=SABispebjerg= and \verb=SAGroup42F13=. This is due to the branching factor on the two, combined with the inherent source of error, from calculating distances, using Manhattan Distance. 

		Applying AStar, however, we see that we are now able to actually solve the previously unsolvable levels. However, the time for it to solve \verb=SABispebjerg= is far from good. Impressively we manage to solve \verb=SAGroup42F13= using only 756.4 ms in average. Having said all this, using the Manhattan distance map, using AStar seems to be the best option.


	\subsection{Distance Maps}
		Having compared AStar and Greedy based on the Manhattan distance map, we can now take another look on the benefit of the distance map. As we've mentioned before, we are applying a FloydWarshall algorithm to generate it. This comes with a trade-off: Time. In terms of running time, the algorithm is bound by $O(|V|^3)$ in pre-computation time, where $V$ is the number of cells. On some levels this trade-off is non-noticeable, simply due to the increased effectiveness. This is very much the case on \verb=SABispebjerg= and \verb=SACrunch=, cf. tables \ref{table:bench_astar} and \ref{table:bench_greedy}. However, when the level reaches a certain size, the time explodes. This is clearly seen on \verb=SAGroupF42F13=.

		One \emph{very} interesting effect of the FloydWarshall distance map, is that it makes the Greedy approach be the optimal approach, cf. table \ref{table:bench_greedy}. This is simply because, that for each location on the level, we know \emph{exactly} how far there is to the goal.

		In the current test set, the only case where we see an actual \emph{increase} in time is on \verb=SAGroup42F13=. A theory as to why the pre-computation time is much more significant on this level, compared to i.e. \verb=SABispebjerg=, is because \verb=SAGroup42F13= can be considered a sparse graph. Because of this, the pre-computation time exceeds the benefit of the reduced search time, in cases like this. While the time penalty \emph{is} a concern in this case, the fact that we increase the quality of the solution by roughly $29\%$, we feel that the FloydWarshall distance map is a good choice, in this domain.

	\subsection{Goal Dependency}

%		Kasper
		Here we will be comparing the effect of running the algorithm with total subgoal independence and cumulative subgoal dependence in the goal conditions. The results of the test can be seen in Tables \ref{table:bench_indygoals} and \ref{table:bench_cumgoals}.
		\begin{table}
			\centering
			\begin{tabular}{ l | r | c | l }
								& 	\multicolumn{3}{c }{Cumulative}							\\
				\textbf{Level}	&	\textbf{Steps}	&	\textbf{Time}	&	\textbf{Rate}	\\
								&					&	\textbf{(s)}	&					\\
				\hline
				SACrunch		&	104				& 	0.13		& 100 \%		\\
				SAnull			&	97.8			& 	0.15		& 100 \%		\\
				SAFirefly		&	97.2			& 	0.13 		& 100 \%	 	\\
				SABispebjerg	&	1526			& 	1.8			& 100 \%		\\
				MAAteam			&	338.6			& 	13.9 		& 100 \%		\\
				MABullFightF11	&	335.0			& 	14.7 		& 100 \%	 	\\
				MAdeepblue		&	252.4			& 	8.2 		& 100 \%	 	\\
				MAnull			&	106.8			& 	8.7			& 100 \%		\\
				
			\end{tabular}
			\caption{Benchmarks using cumulative subgoals.}
			\label{table:bench_indygoals}
		\end{table}

		\begin{table}
			\centering
			\begin{tabular}{ l | r | c | l }
								&	\multicolumn{3}{c}{Independent}		\\
				\textbf{Level}	&	\textbf{Steps}	&	\textbf{Time}	&	\textbf{Rate}	\\				
								&					&	\textbf{(s)}	&					\\
				\hline
				SACrunch		&	102.8		&	0.12	& 	100 \%	\\
				SAnull			&	95			&	0.17	&	20 	\% 	\\
				SAFirefly		& 	123.2		&	0.13	& 	100 \%	\\
				SABispebjerg	&	1523.6		&	1.7		&	100 \% 	\\
				MAAteam			&	228.8		&	6.1		& 	80 	\%	\\
				MABullFightF11	&	303.8		&	8.2		& 	100 \% 	\\
				MAdeepblue		&	229.4		&	8.1		&	100 \% 	\\
				MAnull			&	112.4		&	11.3	& 	100 \% 	\\
				
			\end{tabular}
			\caption{Benchmarks using independent subgoals.}
			\label{table:bench_cumgoals}
		\end{table}


		The results shows a clear difference between the single agent and the multi agent levels. 
		The single agent levels shows that the performance is more or less equal when the looking at steps and time. The only major difference is the success rate one \verb=SAnull=, where a livelock occurs 4/5 of the time, due to the nearer proximity of the goals. This tendency can how ever not be seen on the \verb|SaFirefly| since the agent have a fairly limited set of paths and the agent therefore cant easily move around precious achieved goals and end up undoing them instead, and thereby resulting in a longer solution.
		
		The interesting part of the data is that the independent goal strategy finds faster and shorter solutions for the multi agent levels. This can be explained by the fact that if an agent interferes a with an other agents goal, it is not necessary best to fix the goal immediately, but instead wait for an more opportune time.	The only disadvantage is the same as for the single agent, is that it can end up in livelocks.
		
		Part of the explanation is also, that the heuristic helps avoid destroying already achieved goals, and there by negating most of the disadvantages the independent goal approach has. 
		
		For the multiagent levels \verb|MAnull| breaks the pattern, this is due to the most time consuming part of of the level is practise a minor version of the single agent level \verb|SAboxesOfHanoi| and a performance more similar to pattern from the single agent levels are therefore expected.
	
	
\section{Discussion}
	\label{disscussion}
	With any choice there is bound to be pros and cons: In this particular case, it is primarily the eternal question of the quality of the solution, compared to the time required to generate it. A classic example of this would be, that we originally feared, that performing our double-search would impact our performance in a negative way, what we were happy to discover was that it actually increased it. This is because we effectively terminate any searches, that would otherwise take too long complete. The benefit of this approach was depicted in the results of the competition. 

	One unfortunate thing that turned up during the competition, is that we're only able to successfully solve \verb=SAHoldKaeft= at around $25\%$ success rate. Our investigations show, that this has to do with the way we prioritise goals. The problem is, that we treat the prioritisation as a weight, instead of an actual sequence. The way we've chosen to do it is \emph{fast}, but obviously have certain shortcomings, that in this case unfortunately results in a rather low success rate. 

	Levels such as \verb=SAWatsOn= and \verb=MAmultiagentSort= unfortunately unearths another weakness in our solution. Multiple goals and boxes with the same letter \emph{can} result in the agent moving a box from one goal to another, moving some random box -- as to generate a new state -- and move the box back to its original goal. When caught in this behavior, it will result in a practically infinite search space. The inherent cause of this issue, is that this new ``goal state" is reachable within a single move. Thus when expanding the search space, we will always reach this problematic goal state, no matter what the heuristic returns.

	However, this is just our apparent weaknesses. Our solution also showed to excel at other types of problems. When the level is effectively partitioned into several independent sub-problems, we see how our approach of decomposing the overall problem excels. In this case -- both single- and multi-agent -- we see how we solve the problem quickly, while also obtaining a solution of high quality. Additionally, because multi-agent levels often are highly independent levels, we managed perform goal decomposition to a much greater extent, than we did with single-agent levels. We reckon that this is the primary reason, as to why we perform so much better at multi-agent levels, than we do single-agent levels. One of the method we employ to create this decomposition on the multi-agent levels, is the process of clustering the goals. This is a far more logical process to perform, on these multi-agent levels.

	While we admittedly are in a fully observable world, where a perfect master plan could be devised, we've chosen to rely on relaxation and online planning. This obviously comes with some drawbacks, but also has its advantages. When perform a search to plan a solution, we don't take the other agents into account. This greatly reduces the complexity of the problem, while also simplifying the algorithm. Additionally it also introduces an obvious opportunity of introducing parallelism, to further increase performance. However, taking the restraints of this project into consideration, we chose not go down this path. However, the obvious drawback is that agents inevitably will get in each others ways.
	
\section{Future Work}
	As with any project, there is a deadline. As the deadline approaches, one often finds that there are certain aspects that could still be improved. In this section, we will go through various aspects that could be improved, beyond the deadline of this project. Whether these improvements where not implemented simply due to a lack of time, or not prioritised because their benefit/cost ratio was too low is another story. The improvements can be categorized in two groups, one speeding up performance using more complex algorithms on the already existing approach, the other attacking the problem in another way.
	
	\subsection{Improving the Current Approach} 

		\subsubsection{Distance Maps}
			As we saw in our benchmarks, there \emph{is} a considerable time penalty for using the FloydWarshall distance map on the \verb=SAGroup42F13= level. Originally we chose this particular algorithm, due to it being considerably easier to understand and implement, than other slightly more efficient algorithms. We estimated FloydWarshall to be the best benefit/resource ratio.

			Having said this, if we had had more time, this is definitely a source of improvement. The first step to improving the algorithm, is using Johnson's Algorithm ~\cite{jonhson}, when the level is considered sparse. This effectively reduces the running time from $O(|V|^3)$ to $O(|V|^2log|V|+|V||E|)$, where $|V|$ vertices and $|E|$ edges. But this is far from the best that \emph{could} be done. Mikkel Thorup proposes an algorithm in ~\cite{thorup} which can calculate all shortest paths from a single source in $O(|E|)$ time. This can then be repeated for all sources -- or vertices -- to calculate \emph{all} shortest paths in $O(|E||V|)$. Having said that, the algorithm proposed by Thorup is \emph{far} more complicated than the Floyd-Warshall algorithm.


		\subsubsection{Searching the Nodes}
			During the development of the code, we saw that the standard Java PriorityQueue was not optimal: The \verb=.contains(..)= method -- which is invoked a considerable number of times -- was causing significant slow-downs. This is because of it simply iterating over the queue in $O(n)$ time \cite{website:Java-Docs}. To avoid this, we added a secondary data structure: A hash-set. Using the hash-set we can invoke \verb=.contains(..)= in expected $O(1)$ time.

			However, this comes at a disadvantage: We have to keep \emph{two} data structures in memory, while also maintaining it. Another approach to this, could be to use a Fibonacci Heap ~\cite{fibonacci}. This data structure would allow us to perform most operations at expected time $O(1)$, resulting in a theoretical improvement.

	\subsection{New Approaches}
		Our approach has some general disadvantage as discussed in section \ref{disscussion} and as shown in the competition results the quality of the single agent solution could be higher, which would be obvious to address.

		%\subsubsection{Identical Adjacent Goals:}
			%Box Cycling\\
			%Goal history of which box IDs on it
			%Look into which boxes agent has moved / goals it tried %to achieve.
			%\dots \dots
		\subsubsection{Coordination:}
			We had problems with several agents lack of coordination. Which resulted in agents doing more than necessary noops to avoid conflicts and in general hitting each other even though there are space to pass each other. To avoid this the agent needs to have some idea of where the other agents are at all time. This can be achieved by using a  blackboard approach, and thereby letting the agents modify and validate their plans before executing them.
		\subsubsection{Backtracking:}
		\label{subsub:backtrack}
			Running the complete search offline to allow better backtracking and thereby being able to recover from situations where the agent get stuck by solving the goals in the wrong order. 

		\subsubsection{Goal Decomposition}
			To increase the performance on single agent levels, a more guide approach could be used. This could be by grouping goals together applying the same assumption as used for the multi agent clustering. The difference from multi agent approach would be that the grouping would be that there would be more groupings than agents. A more advanced approach would be to apply the techniques from \cite{Subgoals} to find the ordering of the goals and solve them in the found order. This approach could also be applied in combination with the clustering to the way goals are grouped for the multi agents.
			
			

	\subsection{Generalising}


		%Does your method generalise to other types of domain? If not, what would be required for it to do so?
		
		\subsubsection{Sokoban}
			%Reindahl
			One of the most obvious generalisations would be that of Sokoban or variants of it. Due to it being a very similar domain, and it will therefore be simple to generalise to this domain. The only concern would be that Sokoban does have some irreversible actions/situations (due to no pull action). The concern is that a single error can make a level unsolvable and it would not necessarily be obvious at the time, this means that backtracking is far more important in this domain.

		\subsubsection{Multi-Body NPC Route Finding in a Dynamic Environment}
			% Daniel
			A not quite-so-obvious generalisation, could be to employ our solution for controlling the behaviour of NPCs, in open-world sandbox games.

			Imagine a such a game, where a player is allowed to roam freely around. In these games NPCs usually stroll about, minding their own business, up until their wonderful day is disrupted by a maniac with a Gatling gun. These walking, driving, cycling, or some other form of transportation patterns, could easily be described described using our solution. 

			What makes our solution excellent for this purpose, is that we employ online planning. The player, an outside agent, comes in a disrupts the agents view of the world, by completely eradicating the car in front, using aforementioned Gatling gun. Had we used offline planning, this would be a huge issue. However, with our approach a simple replanning -- or a round of conflict resolution -- suffices to figure out how the NPCs react.


			One aspect where modification would be required, is definitely conflict resolution. As it stands now, NPC-NPC conflicts is solvable, but a more detailed description of Player-NPC conflict would definitely need to be modelled. This, however, could rather easily be achieved by expanding on how we analyse and resolve conflict.


% References and end of paper
\bibliographystyle{aaai}
\bibliography{bibliography}
\end{document}