\def\year{2015}
%File: formatting-instruction.tex
\documentclass[letterpaper]{article}

% Required Packages
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}

\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

% Section numbers. 
\setcounter{secnumdepth}{2}  

\nocopyright
\begin{document}
% Title and author information
\title{How to Kill a Hamster - Group NULL\\ 02285 AI \& MAS}
\author{Daniel Schougaard \\ \textit{s103446} \And Kasper Reindahl Rasmussen\\ \textit{s103476} \And Martin V. Ottesen\\ \textit{s060186}}
\maketitle

\begin{abstract}
The abstract goes here. Please read this document carefully before preparing your manuscript.

To ensure that all reports have a uniform appearance corresponding to published papers at the major AI conferences (like IJCAI and AAAI), authors must adhere to the following instructions. 
\end{abstract}

\section{Introduction}
	Inspired by the developments of robotics used for transporting assignments to lower the need for humans to carry out these trivial tasks.\\ This paper describes a solution to this planing problem on an abstract level, where the hardware of the physical robots have been disregarded and the environments have been relaxed in such a way that they make up grid like patterns. The  moving ententes which will henceforth be referred to as agents are given the ability to push and pull the objects henceforth refereed to as boxes around in the grid map, with the objective of moving these boxes to there destinations called goals, while navigation the obstacles in there way, be they walls, other agents or boxes they do not have the rights to move. By enforcing these rules and relaxations the resulting problem is of a single/multi agent pukoban nature. 
	
	\subsection{The solution}
	To solve the problem the level is first analysed mapping all agents, boxes, goals and walls, calculating distances from any cells to all other cells in the space, clustering the goals for the agents and then prioritizing them so that blocking goals can be avoided.\\
	
	The agents run autonomously with no inter agent communication, the solution is sudo subgoal independent as an agent does not forget old goals when achieved, new ones are simply added to the list. To make sure that a route can always be found a relaxed search is run first and then an actual search. The planing is done on-line so conflicts are detected when the server objects this results in the invocation of conflict resolution.\\
	
	If the case of a single agent problem a simpler strategy is used as all boxes are movable by the agent and no other agents can create road blocks no conflicts can occur.


\section{Introduction}
	Martin
	Like so ~\cite{Pukoban}. 
\section{Background}
	Martin\\
	Theory : Relaxation

\section{Related Work}
	Martin

\section{Methods}
%	\subsection{Brief Overview of Design and Implementation}
%		Reindahl\\
%		the general structure is a multi body approach with preprocessing of the goals.
%		
%		the preprocessing consist of clustering the goals to the agent.
%			Randomization
%		
%		we assume that goals located near each 
%		goal decomposition 
%			subgoal independence
%			adding goals for agents
%		
		The general structure is a multi body approach with preprocessing. with each agent solving its own goals independent of the other agents as far as possible. This is done to lower the complexity of the problem. The complexity is lowered by decomposing the problem by assuming that goals of the same colour, located close to each other are related and therefore can be grouped together (using k-clustering). It is also assumed that solving each grouping is independent of the others. One agent is assigned to solve each grouping. 
		To lower the complexity further for each grouping the goals are solved cumulative. The order of solving the goals is based on a heuristic. It cumulative solves the goals, to avoid destroying already achieved goals, since they are assumed to be dependent on each other. After each search the found plan is executed until it is completed or it fails. if it fails replanning is commenced.

 
				
	\subsection{Search}
		To find a plan to solve a given set of goals, a search is performed. The search is either a Greedy or A* search. A* is used in the single agent world and when the problem is considered small enough, such as moving out of a given path. 
		The search works under the assumption, that nothing changes in the world except what the agent performing the search do.
		
		One thing worth remarking on here is that a randomized approach is used in expanding the search space, which means that a search is often not likely to result in the same plan each time. This even more so the case when combining it with a greedy approach and a non admissible heuristic.


		%		when making a search, in fact two searches is performed
		%		one on a more relaxed world and one less relaxed
		%		
		%		the first search is done on a world where other agent and other wrong colour boxes is removed. this relaxation has no effect in a single system an therefore only one search is performed.
		\subsection{The Heuristic}
			%Reindahl \\
			RELAXATION OF A WABBIT, WABBIT! WABBIT!\\
			a series of heuristics is used, each one to solve different problem. all the heuristic though have that in common, that to enhance the heuristics the distances used, is the shortest path when the domain is relaxed by removing all movable objects, these values can be precomputed (The All-pairs shortest paths problem).
					
			Of all the heuristic used only two are noteworthy \nameref{Goal1} and \nameref{goal2}, the rest is basically just the distance to or from a point.
			\subsubsection{Goal Heuristic}\label{Goal1}
			The main heuristic used is a heuristic used for putting boxes on goals. It consist of two parts, one to ensure progress and one to ensure as little interference as possible with the other agents.
			
			The first part calculates the shortest path from each from each of the agents assigned goals to the agent going through a box that matches the goal (boxes already on goals are not used). all these parts are summed together.
			
			The second part is basically a goal count which works as punishment for moving boxes away from goal fields even though the goals ar not part of the agents goal condition.
			
			this results in an non admissible heuristic.
			
			\subsubsection{Different Goal Heuristic}\label{goal2}
				%Reindahl
				goals are selected from a agents grouping mainly based on the shortest distance from a goal to the nearest similar box (not on a goal) and to the agent. The location of a given goal is also taken into consideration so goals likely to be blocked (fx. dead ends and corners) is prioritised higher by a factor.

	\subsection{Distances in the Map}
		To speed up our search, we have been employing something we've called a ``Distance Map". A distance map is a certain way to calculate distances from one location to another. In the first few iterations, this was merely a wrapper for calculating the Manhattan distance.

		Looking further into this matter, we quickly realized that the levels we work on here, can represented as graphs: Each cell is a vertice, $V$, and each connection to another cell is an edge, $E$. With this knowledge in hand, we quickly realised that we could employ various algorithms actually meant to work on graphs. At the time of writing, we've implemented, tested and worked with a distance map generated using the Floyd-Warshall algorithm.

	\subsection{Conflict Resolution}
		Since we're using multiple agents, each planning their own route, conflicts are bound to happen. We've chosen to rely on a method to resolve these somewhat similar to online planning. But first and foremost, we need to describe exactly how we \emph{detect} these conflicts.

		\subsubsection{Detecting the Conflicts}
			Currently we rely on three different methods for detecting conflicts. The first method is simply letting the server \emph{tell} the client, that there is a conflict. This is by far the simplest possible way to detect a conflict, but it does inevitably result in a greater overhead. 

			The second approach is that we employ a sort of double search. By first relaxing the domain, into something we call a subdomain, only containing the agent itself and boxes, it can move. We then search this reduced domain. Using the length of the found route, we get a threshold for determining whether or not the normal search is stuck.


			%First we relax the domain, removing all other agents and boxes, that the current agent can not move. This results in a new subdomain, in which a search is performed. This results in a sort of threshold: In a perfect world, a solution for this problem exists in $X$ moves. This is then again used, to cut off the normal search, once it has taken too long. When this happens, we assume that it is because of a conflict. Using the route of the relaxed search, we identify which objects are in the way, causing this conflict.

			The third and final approach is using something we call a \verb=History=. Due to the way we implemented searching and re-planning, occasionally two agents could find themselves in a situation, where both of them would \textsc{cycle} between two locations -- a so called live-lock. The \verb=History= is a memory of a certain length, containing information about where an agent has been for the past $x$ moves. Should more than a certain fraction of this history be of the same location, a cycle is detected.

		\subsubsection{Types of Conflicts}
			Having detected a conflict, we partition the causes into one of two groups: The cases where it is only a box in the way, and the cases where it is a box. As a rule, all boxes found in the route, is handled first. This is done to avoid cases, where an agent otherwise asked to move a box, is told to get out of the way. An example of this, can be seen below. $0$ is a \verb=BLUE= agent, and $1$ is a \verb=RED= agent. The box $B$ is also \verb=RED=. 
			\begin{verbatim}
				...+++++++++...
				     0 B1
				...+++++++++...
			\end{verbatim}
			In this example, $0$ would like to move right. Obviously, that is impossible, due to the fact that both the agent $1$ and the box $B$ is in the way. Should we chose to handle agents first, $1$ would move out of the way, leaving no one to move the box $1$.

		\subsubsection{Resolving a Box Conflict}
			To move a box out of the way, we first need to identify which agent should ``help". This is done by first trying to find the nearest agent, which currently does not have a plan. If none is found, the nearest agent is simply hijacked. Once the selected agent has reached the box, we employ a very naïve search: We breadth-first search, until both agent and box is out of the route.

		\subsubsection{Resolving an Agent Conflict}
			If it should happen that is an agent that is in the route, this agent is told to get out of the way, \emph{unless} it's plans length, is below a certain threshold. Should the plan be below this threshold, the agent in the way is allowed to finish its current plan, before getting out of the way \emph{(if it is required at that time)}.

		\subsubsection{Resolving a Cycle}
			A cycle is resolved by simply injecting NoOps in one of the agents, enabling the other to re-plan itself out of the problem area. A cycle conflict falls under the category of an Agent Conflict, but is resolved separately.
		

\section{Experiments and Results}
	Throughout the development and implementation of the client for this project, we have tested out various ideas and designs. This sections contains a mix of experiments performed during all stages of development. The names AStar and Greedy will refer to an A* Search and a Greedy approach search, from now on out.
	
	
	\pagebreak
{
\Huge
UNITS!!!
}
	\begin{table}
		\begin{tabular}{ l | r | l | r | l }
							& 	\multicolumn{2}{c |}{FloydWarshall}	& 	\multicolumn{2}{c}{Manhattan}		\\
			\textbf{Level}	&	\textbf{Steps}	&	\textbf{Time}	&	\textbf{Steps}	&	\textbf{Time}	\\
			\hline
			SACrunch		&	104.0			& 	167.0			&	105.4			&	574.8	\\
			SAFirefly		&	100.4			& 	160.6 			&	62.0			&	202.2	\\
			SABispebjerg	&	1520.8			& 	2023.2 			&	1568.0			&	9439.8	\\
			SAGroup42F13	&	375.0			& 	31846.2 		&	481.4			&	756.4	\\
		\end{tabular}
		\caption{Benchmarks using AStar strategy.}
		\label{table:bench_astar}
	\end{table}


	\begin{table}
		\begin{tabular}{ l | r | l | r | l }
							& 	\multicolumn{2}{c |}{FloydWarshall}	& 	\multicolumn{2}{c}{Manhattan}		\\
			\textbf{Level}	&	\textbf{Steps}	&	\textbf{Time}	&	\textbf{Steps}	&	\textbf{Time}	\\
			\hline
			SACrunch		&	106.4			& 	162.2			&	296.6			&	1849.6	\\
			SAFirefly		&	100.4			& 	164.0 			&	98.4			&	155.8	\\
			SABispebjerg	&	1530.0			& 	1913.6 			&	0.0				&	0.0		\\
			SAGroup42F13	&	375.0			& 	31110.4 		&	0.0				&	0.0		\\
		\end{tabular}
		\caption{Benchmarks using Greedy strategy.}
		\label{table:bench_greedy}
	\end{table}

	\subsection{Testing Methodology}
		All testing is done over the course of 5 iterations, after which the average is calculated. The limits have been set to approximately 2 gigabyte RAM and 5 minutes. The benchmarks in Table \ref{table:bench_astar} and Table \ref{table:bench_greedy} have been run on a system powered by an Intel i5-3570k (Ivy Bridge) processor, equipped with 8 gigabyte 1866MHz DDR3 RAM, running Ubuntu 12.04.
		
		Table \ref{table:bench_goals} have been run on a system powered by an Intel i7-3680QM (Ivy Bridge) processor, equipped with 8 gigabyte 1600MHz DDR3 SDRAM, running Windows 8.1.

	\subsection{Comparison of Approaches}
		First off, it could seem like our benchmarks are flawed: Where Greedy fails to find a solution using the Manhattan distance, AStar actually manages to find it, cf. table \ref{table:bench_greedy}. This might seem a tad counter-intuitive. But the reason is actually fairly simple; We apply goal decomposition, as mentioned earlier. Each search is treated as its own. Hence the difference here, stems from respectively Greedy and AStar fares in search across the level. In larger levels with a rather high possible branching factor, the Greedy approach would simply end up reaching the memory limit, before even getting a box \emph{close} to the goal.

		Having said that, we can conclude using a Manhattan distance, with Greedy is a horrible choice. Not only does it result in sub-par solutions, but it simply fails to solve both \verb=SABispebjerg= and \verb=SAGroup42F13=. This is due to the branching factor on the two, combined with the inherent source of error, from calculating distances, using Manhattan Distance. 

		Applying AStar, however, we see that we are now able to actually solve the previously unsolvable levels. However, the time for it to solve \verb=SABispebjerg= is far from good. Impressively we manage to solve \verb=SAGroup42F13= using only 756.4 ms in average. Having said all this, using the Manhattan distance map, using AStar seems to be the best option.


	\subsection{Distance Maps}
		Having compared AStar and Greedy based on the Manhattan distance map, we can now take another look on the benefit of the distance map. As we've mentioned before, we are applying a FloydWarshall algorithm to generate it. This comes with a trade-off: Time. In terms of running time, the algorithm is bound by $O(|V|^3)$ in pre-computation time, where $V$ is the number of cells. On some levels this trade-off is non-noticeable, simply due to the increased effectiveness. This is very much the case on \verb=SABispebjerg= and \verb=SACrunch=. However, when the level reaches a certain size, the time explodes. This is clearly seen on \verb=SAGroupF42F13=.

		One \emph{very} interesting effect of the FloydWarshall distance map, is that it makes the Greedy approach be the optimal approach. This is simply because, that for each location on the level, we know \emph{exactly} how far there is to the goal.

		In the current test set, the only case where we see an actual \emph{increase} in time is on \verb=SAGroup42F13=. A theory as to why the pre-computation time is much more significant on this level, compared to i.e. \verb=SABispebjerg=, is because \verb=SAGroup42F13= can be considered a sparse graph \cite{sparse}. Because of this, the pre-computation time exceeds the benefit of the reduced search time, in cases like this. While the time penalty \emph{is} a concern in this case, the fact that we increase the quality of the solution by roughly $29\%$, we feel that the FloydWarshall distance map is a good choice, in this domain.

	\subsection{Goals}
%		Kasper
	\begin{table}
		\begin{tabular}{ l | r | c | l | c | r | l }
			& 	\multicolumn{3}{c |}{Cumulative}	& 	\multicolumn{3}{c}{independent}		\\
			\textbf{Level}	&	\textbf{Steps}	&	\textbf{Time}	&	\textbf{rate} &	\textbf{Steps}	&	\textbf{Time}	&	\textbf{rate}\\
			\hline
			SACrunch		&		104		& 	0.13		& 100 \%	&	102.8		&	0.12	& 100 \%	\\
			SAnull			&	97.8			& 	0.15			& 100 \%	&		95	&	0.17	&	20 \% \\
			SAFirefly		&	97.2			& 	0.13 			& 100 \%	& 123.2			&	0.13	& 100 \%	\\
			SABispebjerg	&	1526			& 	 	1.8		& 100 \%	&	1523.6		&	1.7	&	100 \% \\
			MAAteam			&	338.6			& 	13.9 		& 100 \%	&		228.8		&	6.1	& 80 \%	\\
			MABullFightF11	&	335.0			& 	14.7 		& 100 \%	&	303.8			&	8.2	& 100 \% \\
			MAdeepblue	&		252.4		& 	8.2 		& 100 \%	&		229.4		&	8.1	&	100 \% \\
			MAnull	&	106.8			& 	 8.7		& 100 \%	&	112.4	&	11.3	& 100 \% \\
			
		\end{tabular}
		\caption{Benchmarks using independent and dependent subgoals.}
		\label{table:bench_goals}
	\end{table}
	The results shows a clear difference between the single agent and the multi agent levels. 
	The single agent part shows that the performance is more or less equal when the looking at steps and time. the only major difference is the success rate one \verb=SAnull=, where a livelock occurs 4/5 of the time, due to the nearer proximity of the goals.
	
	The interesting part of the data is that the independent goal strategy finds faster and shorter solutions for the multi agent levels. This can be explained by the fact if an agent interferes a with an other agents goal, it is not necessary best to fix the goal immediately, but instead wait for an more opportune time.	The only problem is the same as for the single agent is that it can end up in livelocks.
	
	Part of the explanation is also that the heuristic helps avoid destroying already achieved goals, and there by negating most of the disadvantages the independent goal approach has. 
	
	why is this not the case for MAnull???
	
	
	
\section{Discussion}
	\label{disscussion}
	fully observable world.... full knowledge... complete off-line planning is possible, why not complete offline planing?
	\subsection{Comparison of Levels}
	\begin{itemize}
		\item{Why is this?}
		\item{Large levels!}
		\item{Coordination amongst Agents.}
	\end{itemize}


\section{Future Work}
	As with any project, there is a deadline. As the deadline approaches, one often finds that there are certain aspects that could still be improved. In this section, we will go through various aspects that could be improved, beyond the deadline of this project. Whether these improvements where not implemented simply due to a lack of time, or not prioritised because their benefit/cost ratio was too low is another story. The improvements can be categorized in two groups, one speeding up performance using more complex algorithms on the already existing approach, the other attacking the problem in another way.
	
	\subsection{improving current approach} 

	\subsubsection{Distance Maps}
		As we saw in our benchmarks, there \emph{is} a considerable time penalty for using the FloydWarshall distance map on the \verb=SAGroup42F13= level. Originally we chose this particular algorithm, due to it being considerably easier to understand and implement, than other slightly more efficient algorithms. We estimated FloydWarshall to be the best benefit/resource ratio.

		Having said this, if we had had more time, this is definitely a source of improvement. The first step to improving the algorithm, is using Johnson's Algorithm\cite{jonhson}, when the level is considered sparse. This effectively reduces the running time from $O(|V|^3)$ to $O(|V|^2log|V|+|V||E|)$, where $|V|$ vertices and $|E|$ edges. But this is far from the best that \emph{could} be done. Mikkel Thorup proposes an algorithm in \cite{Thorup} which can calculate all shortest paths from a single source in $O(|E|)$ time. This can then be repeated for all sources -- or vertices -- to calculate \emph{all} shortest paths in $O(|E||V|)$. Having said that, the algorithm proposed by Thorup is \emph{far} more complicated than the Floyd-Warshall algorithm.


	\subsubsection{Searching the Nodes}
		During the development of the code, we saw that the standard Java PriorityQueue was not optimal: The \verb=.contains(..)= method -- which is invoked a considerable number of times -- was causing significant slow-downs. This is because of it simply iterating over the queue in $O(n)$ time. To avoid this, we added a secondary data structure: A hash-set. Using the hash-set we can invoke \verb=.contains(..)= in expected $O(1)$ time.

		However, this comes at a disadvantage: We have to keep \emph{two} data structures in memory, while also maintaining it. Another approach to this, could be to use a Fibonacci Heap \cite{Fibonacci}. This data structure would allow us to perform most operations at expected time $O(1)$, resulting in a theoretical improvement.

	\subsection{New approaches}
	Our approach has three general disadvantage as discussed in section \ref{disscussion}, which would be obvious to address,
	\begin{itemize}
		\item{Large Levels:}
			\dots \dots
		\item{Coordination:}
			improving the coordination between several agents, by using a blackboard approach, this would avoid many of the situations where agents keeps getting in each others way.
		\item{Backtracking:}
			Running the complete search offline to allow better backtracking and thereby being able to recover from situations where we get stuck by solving the goals in the wrong order. 
	\end{itemize}

\section{References}
	Martin\\
	6 references, 3 papers.
		
		
% References and end of paper
\bibliographystyle{aaai}
\bibliography{bibliography}
\end{document}