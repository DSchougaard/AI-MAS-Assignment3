\def\year{2015}
%File: formatting-instruction.tex
\documentclass[letterpaper]{article}

% Required Packages
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

% Section numbers. 
\setcounter{secnumdepth}{2}  

\nocopyright
\begin{document}
% Title and author information
\title{How to Kill a Hamster - Group NULL\\ 02285 AI \& MAS}
\author{Daniel Schougaard \\ \textit{s103446} \And Kasper Reindahl Rasmussen\\ \textit{s103476} \And Martin V. Ottesen\\ \textit{s060186}}
\maketitle

\begin{abstract}
The abstract goes here. Please read this document carefully before preparing your manuscript.

To ensure that all reports have a uniform appearance corresponding to published papers at the major AI conferences (like IJCAI and AAAI), authors must adhere to the following instructions. 
\end{abstract}

\section{Introduction}
	Martin
	Like so ~\cite{book2015}. 
\section{Background}
	Martin\\
	Theory : Relaxation

\section{Related Work}
	Martin

\section{Methods}
	\subsection{Brief Overview of Design and Implementation}
%		Reindahl\\
%		the general structure is a multi body approach with preprocessing of the goals.
%		
%		the preprocessing consist of clustering the goals to the agent.
%			Randomization
%		
%		we assume that goals located near each 
%		goal decomposition 
%			subgoal independence
%			adding goals for agents
%		
		the general structure is a multi body approach with preprocessing. with each agent solving its goals independent of the other agents as far as possible. This is done to lower the complexity of the problem. The complexity is lowered by decomposing the problem by assuming that goals of the same colour, located close to each other are related and therefore can be grouped together. It is also assumed that solving each grouping is independent of the others.  one agent is assigned to solve to each grouping. 
		To lower the complexity of solving each grouping the agent picks a single goal based on a heuristic, and solve this.. it cumulative solves the goals, to avoid destroying already achieved goals, since they are assumed to be dependent on each other. After each search the found plan is executed until it is completed or it fails. if it fails replanning is commenced
		\\
		\\
	
		real distance.....
		to enhance the heuristics the distances used is the shortest path when the domain is relaxed by removing all movable objects
		 
	\subsection{goal Heuristic}
		%Reindahl
		goals are mainly selected from a given grouping based on how close they and the nearest similar box (not on a goal) are to the given agent. the location of a given goal is also taken into consideration so goals likely to be blocked (fx. dead ends and corners) is prioritised higher 
				
	\subsection{Search}
		to find a plan to solve a given set of goals, a search is performed. The search is either a greedy or A* search. A* is used in the single agent world and when the problem is considered small enough, such as moving out of a given path. 
		The search works under the assumption that nothing changes in the world except what the agent performing the search do.
		
%		when making a search, in fact two searches is performed
%		one on a more relaxed world and one less relaxed
%		
%		the first search is done on a world where other agent and other wrong colour boxes is removed. this relaxation has no effect in a single system an therefore only one search is performed.
		\subsubsection{The Heuristic}
			Reindahl \\
			RELAXATION OF A WABBIT
			the main heuristic used is heuristic used for putting boxes on goals.
			the heuristic used consist of two parts, one to ensure progress and one to ensure as little interference as possible with the other agents.
			
			first parts finds the shorts distance from the agent to a box to the goal for each of the agents assigned goals and sums it.
			
			the second part is basically a goal count which works as punishment for moving boxes away from goal fields even though the goals ar not part of the agents goal condition.
			
			not optimistic
			
			
			

	\subsection{Conflict Resolution}
		Since we're using multiple agents, each planning their own route, conflicts are bound to happen. We've chosen to rely on a method to resolve these, somewhat similar to online planning. But first and foremost, we need to describe exactly how we \emph{detect} these conflicts.

		\subsubsection{Detecting the Conflicts}
			Currently we rely on three different methods for detecting conflicts. The first method is simply letting the server \emph{tell} the client, that there is a conflict. This is by far the simplest possible way to detect a conflict, but it does inevitably result in a grater overhead. 

			The second approach is, that we employ a sort of double search. By first relaxing the domain, into something we call a subdomain, only containing the agent itself, and boxes it can move, we then search this domain. Using the length of the found route, we get a threshold, for determining whether or not the normal search is stuck.


			%First we relax the domain, removing all other agents and boxes, that the current agent can not move. This results in a new subdomain, in which a search is performed. This results in a sort of threshold: In a perfect world, a solution for this problem exists in $X$ moves. This is then again used, to cut off the normal search, once it has taken too long. When this happens, we assume that it is because of a conflict. Using the route of the relaxed search, we identify which objects are in the way, causing this conflict.

			The third an final approach is using something we call a \verb=History=. Due to the way we implemented searching and re-planning, occasionally two agents could find themselves in a situation, where both of them would \textsc{cycle} between two locations. The \verb=History= is memory of a certain length, containing information about where an agent has been for the past $x$ moves. Should more than a certain fraction of this history be of the same location, a cycle is detected.

		\subsubsection{Types of Conflicts}
			Having detected a conflict, we partition the causes into one of two groups: The cases where it is only a box in the way, and the cases where it is a box. As a rule, all boxes found in the route, is handled first. This is done to avoid cases, where an agent otherwise asked to move a box, is told to get out of the way. An example of this, can be seen below. $0$ is a \verb=BLUE= agent, and $1$ is a \verb=RED= agent. The box $B$ is also \verb=RED=. 
			\begin{verbatim}
				...+++++++++...
				     0 B1
				...+++++++++...
			\end{verbatim}
			In this example, $0$ would like to move right. Obviously, that is impossible, due to the fact that both the agent $1$ and the box $B$ is in the way. Should we chose to handle agents first, $1$ would move out of the way, leaving no one to move the box $1$.

		\subsubsection{Resolving a Box Conflict}
			To move a box out of the way, we first need to identify which agent should ``help". This is done by first trying to find the nearest agent, which currently does not have a plan. If none is found, the nearest agent is simply hijacked. Once the selected agent has reached the box, we employ a very naïve search: We breadth-first search, until both agent and box is out of the route.

		\subsubsection{Resolving an Agent Conflict}
			If it should happen that is an agent that is in the route, this agent is told to get out of the way, \emph{unless} it's plans length, is below a certain threshold. Should the plan be below this threshold, the agent in the way is allowed to finish its current plan, before getting out of the way \emph{(if it is required at that time)}.

		\subsubsection{Resolving a Cycle}
			A cycle is resolved by simply injecting NoOps in one of the agents, enabling the other to re-plan itself out of the problem area.
		

\section{Experiments and Results}
	Throughout the development and implementation of the client for this project, we have tested out various ideas and designs. This sections contains a mix of experiments performed during all stages of development.

	\subsection{Distance Maps}
		During the early stages of development, we quickly realized that have a sort of modular approach to calculating distances would be beneficial. First few iterations we only employed a Manhattan distance calculation. Later on we then implemented various approaches to generating what we call "Real Distance Maps": They calculate the \emph{actual} distance, between to coordinate sets, by pre-processing.

		All testing is done on single agent levels, since we're testing the efficiency of using the real distance map, instead of a Manhattan distance. To create a fixed point of reference, a greedy approach is used on all benchmarks.
		\begin{tabular}{ | l | l | l | }
			\hline
			\textbf{Level} 	& \textbf{Manhattan} 	& \textbf{Floyd–Warshall} \\
			\hline
			SABispebjerg 	& - 					& - \\
			SAGroup42F13 	& - 					& - \\
			SACrunch 		& - 					& - \\
			SAboxesOfHanoi 	& - 					& - \\
			\hline
		\end{tabular}
		As it is seen, the real distance map, significantly increases the performance of our search client on the test cases, as we suspected.

	\subsection{Comparison of Approaches}
		Daniel
		% Greedy vs AStar

	\subsection{Goals}
		Kasper
		\begin{itemize}
			\item{Individual goals vs series of goals}
			\item{Prioritizing goals}
		\end{itemize}


\section{Discussion}
	fully observable world.... full knowledge... complete off-line planning is possible, why not complete offline planing?
	\subsection{Comparison of Levels}
	\begin{itemize}
		\item{Why is this?}
		\item{Large levels!}
		\item{Coordination amongst Agents.}
	\end{itemize}


\section{Future Work}
	\begin{itemize}
		\item{Large Levels}
		\item{Coordination}
	\end{itemize}

\section{References}
	Martin\\
	6 references, 3 papers.
		
		
% References and end of paper
\bibliographystyle{aaai}
\bibliography{bibliography}
\end{document}